{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20543,
     "status": "ok",
     "timestamp": 1589982117288,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "OSWIsPhh6zD5",
    "outputId": "6a8bc680-fbfc-4a3a-faf1-937ef58551ce"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.1\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1589986143277,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "HrphpbuM7Jfj",
    "outputId": "09db6806-16c3-4e08-c959-06bf4c895279"
   },
   "outputs": [],
   "source": [
    "# !wget -O \"cnn_stories.tgz\" \"https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\"\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\" -O cnn_stories.tgz && rm -rf /tmp/cookies.txt\n",
    "!tar -xvzf \"cnn_stories.tgz\"\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1589984127360,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "LWAUtTpM6ps5",
    "outputId": "06e1113c-83c8-4e55-d543-b16517790cb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import sent_tokenize\n",
    "import pickle\n",
    "from multiprocessing import Pool, Process\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from rouge_score import rouge_scorer\n",
    "import bert\n",
    "\n",
    "\n",
    "def get_sample(doc, sum, scorer, tokenizer):\n",
    "    doc_size = len(doc)\n",
    "    res = np.zeros(doc_size, dtype=\"int32\")\n",
    "    n = min(len(sum), doc_size)\n",
    "    # f1 of rouge-L\n",
    "    for j in range(n):\n",
    "        score = [scorer.score(sum[j], sent_i)['rougeL'][2] for sent_i in doc]\n",
    "        sent_pos = np.argmax(score)\n",
    "        for i in range(doc_size):\n",
    "            if res[sent_pos] == 1:\n",
    "                score[sent_pos] = 0\n",
    "                sent_pos = np.argmax(score)\n",
    "            else:\n",
    "                res[sent_pos] = 1\n",
    "                break\n",
    "        # print(score[sent_pos])\n",
    "        # print(doc[sent_pos])\n",
    "        # print(sum[j], \"\\n\")\n",
    "\n",
    "    doc_ids = []\n",
    "    for sent_j in doc:\n",
    "        if sent_j != '':\n",
    "            doc_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS] \" + sent_j)))\n",
    "        else:\n",
    "            doc_ids.append([0])\n",
    "\n",
    "    return doc_ids, res\n",
    "\n",
    "\n",
    "def pre_processing(data_dir, files, tokenizer, MAX_SEQ_LEN, MAX_DOC_LEN):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    docs_ids = []\n",
    "    labels = []\n",
    "    for i in range(len(files)):\n",
    "        if i%1000 == 0:\n",
    "            print(os.getpid(), i)\n",
    "        with open(os.path.join(data_dir, files[i]), encoding='utf-8') as f:\n",
    "            document = f.read().rstrip().split(\"\\n\\n@highlight\\n\\n\")\n",
    "            summary = document[1:]\n",
    "            doc = document[0].split(\"\\n\\n\")\n",
    "            remove = [i for i in range(len(doc)) if (len(doc[i]) < 20 or (\"EST\" in doc[i] and \"20\" in doc[i]))]\n",
    "            for i in range(len(remove)-1, -1, -1):\n",
    "                doc.pop(i)\n",
    "            if len(doc) == 0 or doc[0] == '':\n",
    "                continue    \n",
    "            if len(doc) > MAX_DOC_LEN:\n",
    "                doc = doc[:MAX_DOC_LEN]\n",
    "            else:\n",
    "                doc += [''] * (MAX_DOC_LEN - len(doc)) \n",
    "            doc_ids, label = get_sample(doc, summary, scorer, tokenizer)\n",
    "            docs_ids.append(pad_sequences(doc_ids, maxlen=MAX_SEQ_LEN, dtype='int32',\n",
    "                                            padding='post', truncating='post'))\n",
    "            labels.append(label)\n",
    "    return docs_ids, labels\n",
    "\n",
    "\n",
    "def write(write_dir, data, name, is_x_train=True):\n",
    "    if not os.path.isdir(write_dir + \"/x_train/\"):\n",
    "        os.mkdir(write_dir + \"/x_train/\")\n",
    "        os.mkdir(write_dir + \"/y_train/\")\n",
    "    if is_x_train:\n",
    "        with open(write_dir + \"/x_train/\" + name, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "    else:\n",
    "        with open(write_dir + \"/y_train/\" + name, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def preprocess_and_write(inp_list):\n",
    "    data_dir, write_dir, files, vocab_file, MAX_SEQ_LEN, MAX_DOC_LEN, name = inp_list\n",
    "    tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file)\n",
    "    docs_ids, labels = pre_processing(data_dir, files, tokenizer, MAX_SEQ_LEN, MAX_DOC_LEN)\n",
    "    write(write_dir, docs_ids, name, True)\n",
    "    write(write_dir, labels, name, False)\n",
    "    return docs_ids, labels\n",
    "\n",
    "\n",
    "def read(inp_list):\n",
    "    read_dir, name = inp_list\n",
    "    with open(read_dir + \"/x_train/\" + name, \"rb\") as f:\n",
    "        x_train = pickle.load(f)\n",
    "    with open(read_dir + \"/y_train/\" + name, \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def parallel(func, input_list, num_workers=4):\n",
    "    with Pool(num_workers) as p:\n",
    "        res = p.map(func, input_list)\n",
    "    return [i for i in res]\n",
    "\n",
    "\n",
    "def sequence(func, input_list):\n",
    "    res = []\n",
    "    for inp in input_list:\n",
    "        res.append(func(inp))\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = \"cnn/stories\"\n",
    "    WRITE_DIR = \"train_data\"\n",
    "    files = os.listdir(DATA_DIR)\n",
    "    num_files = len(files)\n",
    "    print(num_files)\n",
    "\n",
    "    do_lower_case = True\n",
    "    vocab_file = os.path.join(model_dir, \"vocab.txt\")\n",
    "    tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "    MAX_SEQ_LEN = 128\n",
    "    MAX_DOC_LEN = 64\n",
    "\n",
    "    np.random.shuffle(files)\n",
    "    if not os.path.isdir(WRITE_DIR):\n",
    "        os.mkdir(WRITE_DIR)\n",
    "    with open(WRITE_DIR + \"/files_0\", \"wb\") as f:\n",
    "        pickle.dump(files, f)\n",
    "\n",
    "        \n",
    "    NUM_WORKERS = 4\n",
    "    write_task_input = [[DATA_DIR, WRITE_DIR, files[num_files*i//NUM_WORKERS: num_files*(i+1)//NUM_WORKERS],\n",
    "                         vocab_file, MAX_SEQ_LEN, str(i)] for i in range(0, NUM_WORKERS)]\n",
    "    \n",
    "    parallel(preprocess_and_write, write_task_input, NUM_WORKERS)\n",
    "    sequence(preprocess_and_write, write_task_input)\n",
    "    data_len = 15000\n",
    "#     sequence(preprocess_and_write, [[DATA_DIR, WRITE_DIR, files[:data_len],\n",
    "#                          vocab_file, MAX_SEQ_LEN, MAX_DOC_LEN, \"data\"]])\n",
    "\n",
    "    read_task_input = [[WRITE_DIR, str(i)] for i in np.arange(0, NUM_WORKERS*INTERVAL, INTERVAL)]\n",
    "    print(parallel(read, read_task_input, NUM_WORKERS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2488,
     "status": "ok",
     "timestamp": 1589984151557,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "OAp26PQT6ps_",
    "outputId": "c33add20-4fc9-4e2c-d5c1-adf5d819a4c2"
   },
   "outputs": [],
   "source": [
    "# !cp -a \"./train_data\" \"drive/My Drive/project 2/Data/Daily_Mail\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, y_train = read([\"./train_data\", \"data\"])\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = x_train[i][:MAX_DOC_LEN]\n",
    "    y_train[i] = y_train[i][:MAX_DOC_LEN]\n",
    "lengths = [len(i) for i in y_train]\n",
    "# x_test, y_test = x_train[:1000], y_train[:1000]\n",
    "# x_train, y_train = x_train[1000:], y_train[1000:]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "     test_size=1000/len(x_train), random_state=2)\n",
    "print(np.mean(lengths), np.std(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzngBzzi6ptF"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Bidirectional, Flatten, \\\n",
    "    GRU, Input, GlobalAveragePooling2D, Dense, Conv2D, MaxPool2D, \\\n",
    "    GlobalAveragePooling1D\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid, softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self, MAX_DOC_LEN, MAX_SEQ_LEN, layer_bert):\n",
    "        #  base on architecture of \"Ranking Sentences for Extractive Summarization\"\n",
    "#         self.bert_input_shape = (MAX_DOC_LEN, MAX_SEQ_LEN)\n",
    "        self.input_shape = (MAX_DOC_LEN, MAX_SEQ_LEN, 128)\n",
    "        self.bert = layer_bert\n",
    "        self.pooled_bert = GlobalAveragePooling1D()\n",
    "        self.document_encoder_hidden = 96\n",
    "        self.cnn_output_dim = 32\n",
    "        self.sentence_extractor_hidden = self.document_encoder_hidden + self.cnn_output_dim\n",
    "        self.loss = BinaryCrossentropy()\n",
    "        self.optimizer = Adam(0.001)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # input\n",
    "        inp_cnn = Input(self.input_shape)  # (None, MAX_DOC_LEN, MAX_SEQ_LEN, 128)\n",
    "        inp_sum = Input((MAX_DOC_LEN, MAX_SEQ_LEN))  # (None, MAX_DOC_LEN, MAX_SEQ_LEN)\n",
    "        \n",
    "        # document_encoder and sentence_extractor\n",
    "        document_encoder = GRU(self.document_encoder_hidden, go_backwards=True)\n",
    "        sentence_extractor = GRU(self.sentence_extractor_hidden, return_sequences=True)\n",
    "        \n",
    "        # CNN for extract features\n",
    "        cnn1 = Conv2D(64, (4, 4), activation=\"relu\", padding=\"same\")\n",
    "        maxpool1 = MaxPool2D((4, 4))\n",
    "        cnn2 = Conv2D(16, (4, 4), activation=\"relu\", padding=\"same\")\n",
    "        maxpool2 = MaxPool2D((2, 2))\n",
    "        \n",
    "        # cnn flow\n",
    "        features = cnn1(inp_cnn)  # 64x128x64\n",
    "        features = maxpool1(features)  # 16x32x64\n",
    "        features = cnn2(features)  # 16x32x32\n",
    "        features = maxpool2(features)  # 8x16x16\n",
    "        features = Flatten()(features)  # 2048\n",
    "        features = Dense(self.cnn_output_dim)(features)  # (None, self.cnn_output_dim )\n",
    "        \n",
    "        # main flow\n",
    "        doc_encoder_out = document_encoder(inp_sum)  # (None, self.document_encoder_hidden)\n",
    "        sent_extractor_intitail_state = tf.concat([doc_encoder_out, features], axis=1)\n",
    "        out = sentence_extractor(inp_sum, initial_state=sent_extractor_intitail_state)  # (None, MAX_DOC_LEN, self.sentence_extractor_hidden)\n",
    "        out = GlobalAveragePooling1D(data_format='channels_first')(out)  # (None, MAX_DOC_LEN)\n",
    "        out = Dense(MAX_DOC_LEN, activation='sigmoid')(out)\n",
    "        \n",
    "        model = Model([inp_sum, inp_cnn], out)\n",
    "        model.compile(optimizer=self.optimizer,\n",
    "                       loss=self.loss)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "\n",
    "    def get_inp(self, inputs):\n",
    "        inp_sums = []\n",
    "        inp_cnns = []\n",
    "        for inp in inputs:\n",
    "            inp_cnn = self.bert(tf.convert_to_tensor(inp))\n",
    "            inp_cnns.append(inp_cnn)\n",
    "            inp_sums.append(self.pooled_bert(inp_cnn))\n",
    "        return [tf.convert_to_tensor(inp_sums), tf.convert_to_tensor(inp_cnns)]\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        inps = self.get_inp(inputs)\n",
    "        return self.model.predict(inps)\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=4):\n",
    "        losses = []\n",
    "        mean_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            inp = self.get_inp(x_train[i: i+batch_size])\n",
    "            loss = np.mean(self.model.train_on_batch(inp, np.array(y_train[i: i+batch_size])))           \n",
    "            mean_loss = (mean_loss*i + loss*batch_size) / (i+batch_size)\n",
    "            print(\"step: {},  loss: {}\".format(i,mean_loss))\n",
    "            losses.append(loss)\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def save(self, dir):\n",
    "        if not os.path.isdir(dir):\n",
    "            os.mkdir(dir)\n",
    "        self.model.save_weights(os.path.join(dir, \"model_weights.h5\"))\n",
    "                  \n",
    "    def load(self, dir):\n",
    "        if not os.path.isdir(dir):\n",
    "            print(\"Path does not exist!\")\n",
    "            return None\n",
    "        self.model.load_weights(os.path.join(dir, \"model_weights.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZBgdZt96ptH"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_dir = \"uncased_L-2_H-128_A-2\"\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n",
    "\n",
    "    model_ckpt = os.path.join(model_dir, \"bert_model.ckpt\")\n",
    "    MAX_SEQ_LEN = 128\n",
    "    MAX_DOC_LEN = 64\n",
    "\n",
    "    # do_lower_case = not (model_name.find(\"cased\") == 0 or model_name.find(\"multi_cased\") == 0)\n",
    "    do_lower_case = True\n",
    "    bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case, model_ckpt)\n",
    "    vocab_file = os.path.join(model_dir, \"vocab.txt\")\n",
    "    tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "    bert_params = bert.params_from_pretrained_ckpt(model_dir)\n",
    "    bert_layer = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1677,
     "status": "ok",
     "timestamp": 1589984157847,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "frVkylq76ptJ",
    "outputId": "3a84a9c7-d812-4e69-9d08-088596cd5580"
   },
   "outputs": [],
   "source": [
    "extractor = Extractor(MAX_DOC_LEN, MAX_SEQ_LEN, bert_layer)\n",
    "# extractor.load(\"drive/My Drive/project 2/save/cnn\")\n",
    "print(extractor.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 625799,
     "status": "ok",
     "timestamp": 1589984787458,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "pva3GiJ16ptM",
    "outputId": "396bcf7f-77c8-4b7d-c36c-d3d2e349c23c"
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# x_train, y_train = shuffle(x_train, y_train)\n",
    "batch_size = 32\n",
    "losses = extractor.train(x_train, y_train, batch_size)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1589984800153,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "YYYd6Ja66ptP",
    "outputId": "e5557d47-ffb1-49f3-fb8b-ab11c3ffe1b7"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "size = len(losses)\n",
    "print(len(losses))\n",
    "step = np.arange(0, len(losses))\n",
    "plt.plot(step, losses)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"step (x{})\".format(size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwkI2b6sHT6S"
   },
   "outputs": [],
   "source": [
    "extractor.save(\"save/cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113337,
     "status": "ok",
     "timestamp": 1589984941497,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "aKtHHOj86ptR",
    "outputId": "21bbca6a-23b3-4f9c-e525-3334ac74e31d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import sent_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "def predict(data_dir, tokenizer, extractor):\n",
    "    with open(data_dir, encoding='utf-8') as f:\n",
    "        document = f.read().rstrip().split(\"\\n\\n@highlight\\n\\n\")\n",
    "        summary = document[1:]\n",
    "        doc = document[0].split(\"\\n\\n\")\n",
    "        remove = [i for i in range(len(doc)) if (len(doc[i]) < 20 or (\"EST\" in doc[i] and \"20\" in doc[i]))]\n",
    "        for i in range(len(remove)-1, -1, -1):\n",
    "            doc.pop(i)\n",
    "        if len(doc) == 0 or doc[0] == \"\":\n",
    "            return None, None\n",
    "        if len(doc) > MAX_DOC_LEN:\n",
    "            doc = doc[:MAX_DOC_LEN]\n",
    "        else:\n",
    "            doc += [''] * (MAX_DOC_LEN - len(doc))\n",
    "        doc_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS] \" + sent_j)) for sent_j in doc]\n",
    "        doc_ids = pad_sequences(doc_ids, maxlen=MAX_SEQ_LEN, dtype='int32',\n",
    "                                padding='post', truncating='post')\n",
    "    out = softmax(tf.convert_to_tensor(extractor.predict([doc_ids])))[0]\n",
    "    sent_pos = sorted(sorted(range(len(out)), key = lambda sub: out[sub])[-4:])\n",
    "    return [doc[i] for i in sent_pos], summary\n",
    "\n",
    "# def cal_rouge(sum, doc):\n",
    "#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "#     return scorer.score(sum, doc)\n",
    "\n",
    "\n",
    "def eval(data_dir, files, tokenizer, extractor):\n",
    "    fs = files\n",
    "    res = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "    for f in fs:\n",
    "        print(f)\n",
    "        doc, sum = predict(os.path.join(data_dir, f), tokenizer, extractor)\n",
    "        if doc is None:\n",
    "            continue\n",
    "        pos = []\n",
    "        for i in range(len(sum)):\n",
    "            sent_sum = sum[i]\n",
    "            score = [scorer.score(sent_sum, sent)['rougeL'][2] for sent in doc]\n",
    "            mx = np.argmax(score)\n",
    "            for j in range(len(doc)):\n",
    "                if mx not in pos:\n",
    "                    pos.append(mx)\n",
    "                    break\n",
    "                else:\n",
    "                    score[mx] = -999\n",
    "                    mx = np.argmax(score)\n",
    "        # x_score = []\n",
    "        # for sent in doc:\n",
    "        #     x_score.append(max([scorer.score(sum_i, sent)['rougeL'][2] for sum_i in sum]))\n",
    "        # x_score = np.mean(x_score)\n",
    "        doc = '\\n'.join([doc[i] for i in pos])\n",
    "#         doc = '\\n'.join(doc)\n",
    "        sum = '\\n'.join(sum)\n",
    "        score = scorer.score(sum, doc)\n",
    "        print(score['rougeLsum'][2])\n",
    "        res.append([score['rouge1'][2],\n",
    "                    score['rouge2'][2],\n",
    "                    score['rougeLsum'][2],\n",
    "#                     score['rougeL'][2]\n",
    "                    # , x_score\n",
    "                    ])\n",
    "    print(np.mean(np.array(res, dtype=\"float32\"), 0))\n",
    "    return res\n",
    "\n",
    "res = eval(DATA_DIR, files[70000: 70500], tokenizer, extractor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1586960839634,
     "user": {
      "displayName": "Tan Hoang",
      "photoUrl": "",
      "userId": "16048901001195482969"
     },
     "user_tz": -420
    },
    "id": "D23_Tc2Z6ptT",
    "outputId": "9d2904e8-a2e5-48eb-d434-1899d550b3de"
   },
   "outputs": [],
   "source": [
    "# R1 R2 RL \n",
    "# [0.30686525 0.12081952 0.27703723 ]\n",
    "# [0.31640592 0.12772253 0.28809837 ]\n",
    "print(len(res))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "new_cnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
