{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Header"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install rouge-score\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\" -O cnn_stories.tgz && rm -rf /tmp/cookies.txt\n",
    "# !tar -xzf \"cnn_stories.tgz\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import json\n",
    "import os\n",
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# url list from https://github.com/abisee/cnn-dailymail\n",
    "with open('data/cnndm/filenames/cnn_files.json') as f:\n",
    "    filenames = json.load(f)\n",
    "\n",
    "train_files = filenames['train']\n",
    "valid_files = filenames['valid']\n",
    "test_files = filenames['test']\n",
    "\n",
    "\n",
    "# stanza.download(lang='en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f3356c7d8b34320a31a45c640bdb1eb"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-16 13:46:52 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aed9aa12bd1544eaafb619a1838a0539"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading http://nlp.stanford.edu/software/stanza/1.2.2…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-16 13:48:23 INFO: Finished downloading models and saved to /home/aimenext/stanza_resources.\n",
      "2021-09-16 13:48:23 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-09-16 13:48:23 INFO: Use device: gpu\n",
      "2021-09-16 13:48:23 INFO: Loading: tokenize\n",
      "2021-09-16 13:48:34 INFO: Done loading processors!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "LOWER = False\n",
    "LENGTH_THRESHOLD = 10\n",
    "rouge_factors = {'rouge1': 0.4, 'rouge2': 0.3, 'rougeL': 0.3}  \n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    doc = nlp(doc)\n",
    "    sentences = []\n",
    "    for sentence in doc.sentences:\n",
    "        # print(sentence.tokens[0])\n",
    "        sentence = ' '.join([token.text for token in sentence.tokens])\n",
    "        if len(sentence) > LENGTH_THRESHOLD:\n",
    "            sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def reconstruct_text(text):\n",
    "    return re.sub('\\s([?.!\"](?:\\s|$))', '', text)\n",
    "\n",
    "def parse_file(file):\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        document = f.read().rstrip().split(\"\\n\\n@highlight\\n\\n\")\n",
    "    summary = document[1:]\n",
    "    doc = sent_tokenize(document[0])\n",
    "    return doc, summary\n",
    "\n",
    "\n",
    "def make_label(doc, sum, scorer):\n",
    "    doc_size = len(doc)\n",
    "    res = [0] * doc_size\n",
    "    n = min(len(sum), doc_size)\n",
    "    # f1 of rouge-L\n",
    "    for j in range(n):\n",
    "        # score = [scorer.score(sum[j], sent_i)['rouge2'][2] for sent_i in doc]\n",
    "        score = [scorer.score(sum[j], sent_i) for sent_i in doc]\n",
    "        score = [( \n",
    "            # x['rouge1'][2] * rouge_factors['rouge1'] + \\\n",
    "            x['rouge2'][2] * rouge_factors['rouge2'] + \\\n",
    "            x['rougeL'][2] * rouge_factors['rougeL']\n",
    "            ) for x in score]\n",
    "        sent_pos = np.argmax(score)\n",
    "        for i in range(doc_size):\n",
    "            if res[sent_pos] == 1:\n",
    "                score[sent_pos] = 0\n",
    "                sent_pos = np.argmax(score)\n",
    "            else:\n",
    "                res[sent_pos] = 1\n",
    "                break\n",
    "        # print(score[sent_pos])\n",
    "        # print(doc[sent_pos])\n",
    "        # print(sum[j], \"\\n\")\n",
    "    return res\n",
    "\n",
    "def process(data_dir, files):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    docs = {}\n",
    "    summaries = {}\n",
    "    labels = {}\n",
    "    remove_files = []\n",
    "    for idx in tqdm(range(len(files))):\n",
    "        # if idx%1000 == 0:\n",
    "        #     print('\\n', os.getpid(), idx)\n",
    "        doc, summary = parse_file(os.path.join(data_dir, files[idx]))\n",
    "        if len(doc) < len(summary) or len(doc) == 0 or len(summary) == 0:\n",
    "            remove_files.append(files[idx])   \n",
    "            continue    \n",
    "        label = make_label(doc, summary, scorer)\n",
    "        docs[files[idx]] = doc\n",
    "        labels[files[idx]] = label\n",
    "        summaries[files[idx]] = summary\n",
    "        # if idx%5000 == 0:\n",
    "        #     a = list(zip(label, doc))\n",
    "        #     for i in a:\n",
    "        #         print(len(i[1]), i[0], i[1])\n",
    "        #     print('##########\\n','\\n'.join(summary))\n",
    "    return docs, labels, summaries, remove_files\n",
    "\n",
    "def json_dump(obj, file):\n",
    "    with open(file, 'w', encoding='utf8') as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def process_and_write(data_dir, files, write_dir):\n",
    "    docs, labels, summaries, remove_files = process(data_dir, files)\n",
    "\n",
    "    os.makedirs(write_dir, exist_ok=True)\n",
    "    json_dump(docs, os.path.join(write_dir, 'docs.json'))\n",
    "    json_dump(labels, os.path.join(write_dir, 'labels.json'))\n",
    "    json_dump(summaries, os.path.join(write_dir, 'summaries.json'))\n",
    "    json_dump(remove_files, os.path.join(write_dir, 'remove_files.json'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# base_write_dir = 'data/cnndm/cnn'\n",
    "# process_and_write('cnn/stories', valid_files, os.path.join(base_write_dir, 'valid'))\n",
    "# process_and_write('cnn/stories', train_files, os.path.join(base_write_dir, 'train'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Config:\n",
    "    max_seq_len = 64\n",
    "    max_doc_len = 32\n",
    "    device = 'cuda:0'\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "001563a95dc8b565e74778be5986e3895a5b5171a6836a45578c2beb3674daaa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}