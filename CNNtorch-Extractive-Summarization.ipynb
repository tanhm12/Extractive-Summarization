{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install rouge-score\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\" -O cnn_stories.tgz && rm -rf /tmp/cookies.txt\n",
    "# !tar -xzf \"cnn_stories.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 11:27:09.181600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64:\n",
      "2021-10-02 11:27:09.181628: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch \n",
    "import time\n",
    "\n",
    "from nltk.tokenize import sent_tokenize as nltk_sent_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import LSTM, Conv2d, Linear\n",
    "from torch.nn.functional import max_pool2d\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3356c7d8b34320a31a45c640bdb1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 13:46:52 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed9aa12bd1544eaafb619a1838a0539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading http://nlp.stanford.edu/software/stanza/1.2.2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 13:48:23 INFO: Finished downloading models and saved to /home/aimenext/stanza_resources.\n",
      "2021-09-16 13:48:23 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-09-16 13:48:23 INFO: Use device: gpu\n",
      "2021-09-16 13:48:23 INFO: Loading: tokenize\n",
      "2021-09-16 13:48:34 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# url list from https://github.com/abisee/cnn-dailymail\n",
    "with open('data/cnndm/filenames/cnn_files.json') as f:\n",
    "    filenames = json.load(f)\n",
    "\n",
    "train_files = filenames['train']\n",
    "valid_files = filenames['valid']\n",
    "test_files = filenames['test']\n",
    "\n",
    "\n",
    "# stanza.download(lang='en')\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER = False\n",
    "LENGTH_THRESHOLD = 10\n",
    "rouge_factors = {'rouge1': 0.4, 'rouge2': 0.3, 'rougeL': 0.3}  \n",
    "\n",
    "## This is stanza tokenizer, it is more accurate but much slower than nltk tokenizer\n",
    "# def sent_tokenize(doc):\n",
    "#     doc = nlp(doc)\n",
    "#     sentences = []\n",
    "#     for sentence in doc.sentences:\n",
    "#         # print(sentence.tokens[0])\n",
    "#         sentence = ' '.join([token.text for token in sentence.tokens])\n",
    "#         if len(sentence) > LENGTH_THRESHOLD:\n",
    "#             sentences.append(sentence)\n",
    "    \n",
    "#     return sentences\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    return nltk_sent_tokenize(doc)\n",
    "\n",
    "def reconstruct_text(text):\n",
    "    return re.sub('\\s([?.!\"](?:\\s|$))', '', text)\n",
    "\n",
    "def parse_file(file):\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        document = f.read().rstrip().split(\"\\n\\n@highlight\\n\\n\")\n",
    "    summary = document[1:]\n",
    "    doc = sent_tokenize(document[0])\n",
    "    return doc, summary\n",
    "\n",
    "\n",
    "def make_label(doc, sum, scorer):\n",
    "    doc_size = len(doc)\n",
    "    res = [0] * doc_size\n",
    "    n = min(len(sum), doc_size)\n",
    "    # f1 of rouge-L\n",
    "    for j in range(n):\n",
    "        # score = [scorer.score(sum[j], sent_i)['rouge2'][2] for sent_i in doc]\n",
    "        score = [scorer.score(sum[j], sent_i) for sent_i in doc]\n",
    "        score = [( \n",
    "            # x['rouge1'][2] * rouge_factors['rouge1'] + \\\n",
    "            x['rouge2'][2] * rouge_factors['rouge2'] + \\\n",
    "            x['rougeL'][2] * rouge_factors['rougeL']\n",
    "            ) for x in score]\n",
    "        sent_pos = np.argmax(score)\n",
    "        for i in range(doc_size):\n",
    "            if res[sent_pos] == 1:\n",
    "                score[sent_pos] = 0\n",
    "                sent_pos = np.argmax(score)\n",
    "            else:\n",
    "                res[sent_pos] = 1\n",
    "                break\n",
    "        # print(score[sent_pos])\n",
    "        # print(doc[sent_pos])\n",
    "        # print(sum[j], \"\\n\")\n",
    "    return res\n",
    "\n",
    "def process(data_dir, files):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    docs = {}\n",
    "    summaries = {}\n",
    "    labels = {}\n",
    "    remove_files = []\n",
    "    for idx in tqdm(range(len(files))):\n",
    "        # if idx%1000 == 0:\n",
    "        #     print('\\n', os.getpid(), idx)\n",
    "        doc, summary = parse_file(os.path.join(data_dir, files[idx]))\n",
    "        if len(doc) < len(summary) or len(doc) == 0 or len(summary) == 0:\n",
    "            remove_files.append(files[idx])   \n",
    "            continue    \n",
    "        label = make_label(doc, summary, scorer)\n",
    "        docs[files[idx]] = doc\n",
    "        labels[files[idx]] = label\n",
    "        summaries[files[idx]] = summary\n",
    "        # if idx%5000 == 0:\n",
    "        #     a = list(zip(label, doc))\n",
    "        #     for i in a:\n",
    "        #         print(len(i[1]), i[0], i[1])\n",
    "        #     print('##########\\n','\\n'.join(summary))\n",
    "    return docs, labels, summaries, remove_files\n",
    "\n",
    "def json_dump(obj, file):\n",
    "    with open(file, 'w', encoding='utf8') as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def process_and_write(data_dir, files, write_dir):\n",
    "    docs, labels, summaries, remove_files = process(data_dir, files)\n",
    "\n",
    "    os.makedirs(write_dir, exist_ok=True)\n",
    "    json_dump(docs, os.path.join(write_dir, 'docs.json'))\n",
    "    json_dump(labels, os.path.join(write_dir, 'labels.json'))\n",
    "    json_dump(summaries, os.path.join(write_dir, 'summaries.json'))\n",
    "    json_dump(remove_files, os.path.join(write_dir, 'remove_files.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_write_dir = 'data/cnndm/cnn'\n",
    "process_and_write('cnn/stories', valid_files, os.path.join(base_write_dir, 'valid'))\n",
    "process_and_write('cnn/stories', train_files, os.path.join(base_write_dir, 'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNConfig:\n",
    "    def __init__(self):\n",
    "        self.train_data_dir = 'data/cnndm/cnn/train'\n",
    "        self.val_data_dir = 'data/cnndm/cnn/valid'\n",
    "        self.test_data_dir = 'data/cnndm/cnn/test'\n",
    "\n",
    "        self.bert_type = BertModel\n",
    "        self.tokenizer_type = BertTokenizerFast\n",
    "        self.bert_name = 'prajjwal1/bert-small'\n",
    "\n",
    "        self.MAX_SEQ_LEN = 128\n",
    "        self.MAX_DOC_LEN = 48\n",
    "\n",
    "        self.bert_hidden = 512\n",
    "        self.bert_n_layers = 4\n",
    "\n",
    "        self.windows_size = [1, 3, 5, 10]\n",
    "        self.out_channels = 50\n",
    "        self.lstm_hidden = 256\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 6\n",
    "        self.warmup_steps = 500\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        self.print_freq = 0.05\n",
    "        self.save_dir = './save/cnn'\n",
    "\n",
    "config = CNNConfig()\n",
    "# config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = config.tokenizer_type.from_pretrained(config.bert_name)\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_text(dir, return_sum=False):\n",
    "    docs = load_json(os.path.join(dir, 'docs.json'))\n",
    "    labels = load_json(os.path.join(dir, 'labels.json'))\n",
    "    if return_sum:\n",
    "        summaries = load_json(os.path.join(dir, 'summaries.json'))\n",
    "        return docs, labels, summaries\n",
    "    return docs, labels\n",
    "\n",
    "def get_encodings(docs, labels, config=config):\n",
    "    keys = list(docs.keys())\n",
    "    encodings = []\n",
    "    return_labels = []\n",
    "\n",
    "    for k in tqdm(keys):\n",
    "        encodings.append(tokenizer(docs[k][:config.MAX_DOC_LEN], truncation=True,\n",
    "                                   max_length=config.MAX_SEQ_LEN, padding='max_length'))\n",
    "        return_labels.append(labels[k][:config.MAX_DOC_LEN])\n",
    "    \n",
    "    return keys, encodings, return_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None, keys=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.keys = keys\n",
    "        self.encoding_keys = ['input_ids', 'attention_mask']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(self.encodings[idx][key]) for key in self.encoding_keys}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "def collate_fn(data):\n",
    "    keys = data[0].keys()\n",
    "\n",
    "    result = {k: [item[k] for item in data] for k in keys}\n",
    "    input_ids = result['input_ids']\n",
    "    result['document_mask'] = [torch.tensor([1] * len(input_ids[i])) for i in range(len(input_ids))]\n",
    "    \n",
    "    for k in result:\n",
    "        if k != 'labels':\n",
    "            result[k] = pad_sequence(result[k], batch_first=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_dict_labels = load_text(config.train_data_dir)\n",
    "val_texts, val_dict_labels = load_text(config.val_data_dir)\n",
    "\n",
    "train_keys, train_encodings, train_labels = get_encodings(train_texts, train_dict_labels)\n",
    "val_keys, val_encodings, val_labels = get_encodings(val_texts, val_dict_labels)\n",
    "\n",
    "dataset = ESDataset(val_encodings, val_labels, val_keys)\n",
    "val_loader = DataLoader(dataset, batch_size=3, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('prajjwal1/bert-small')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Embedding(nn.Module):\n",
    "    def __init__(self, bert, config=config):\n",
    "        super(Bert_Embedding, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.bert_hidden = config.bert_hidden * config.bert_n_layers\n",
    "        self.get_n_layers = config.bert_n_layers\n",
    "        self.config = config\n",
    "        \n",
    "        self.windows_size = config.windows_size\n",
    "        self.out_channels = config.out_channels\n",
    "        self.lstm_embedding_size = len(self.windows_size) * config.MAX_SEQ_LEN  \n",
    "        self.filters = nn.ModuleList([nn.Conv2d(1, self.out_channels,\n",
    "                                                (i, self.bert_hidden)) for i in self.windows_size])\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, document_mask, attention_mask):\n",
    "        lens = [mask_i.sum().item() for mask_i in document_mask]\n",
    "\n",
    "        batch, doc_len, seq_len = list(x.shape)\n",
    "        x = x.reshape((batch*doc_len, seq_len))\n",
    "        attention_mask = attention_mask.reshape((batch*doc_len, seq_len))        \n",
    "\n",
    "        last_hds, pooler_output, hidden_states = self.bert(x, attention_mask, output_hidden_states=True)\n",
    "        embeddings = torch.cat(hidden_states[-self.get_n_layers:], axis=-1)  # batch, doc_len, seq_len, self.bert_hidden\n",
    "        # print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape((batch * doc_len, 1,  seq_len, self.bert_hidden))  # batch * doc_len, 1, MAX_SEQ_LEN, bert_hidden\n",
    "        lstm_inputs = []\n",
    "\n",
    "        for i in range(len(self.windows_size)):\n",
    "            temp_out = self.filters[i](embeddings).squeeze(-1)  # batch * doc_len, self.out_channels, MAX_SEQ_LEN - self.windows_size[i] + 1\n",
    "            cnn_result = torch.mean(temp_out, dim=1) # average along out_channels axis\n",
    "            if cnn_result.shape[1] < self.config.MAX_SEQ_LEN: # pad cnn_result to MAX_SEQ_LEN\n",
    "                pad_tensor = torch.zeros((cnn_result.shape[0], self.config.MAX_SEQ_LEN - cnn_result.shape[1])).to(cnn_result.device)\n",
    "                cnn_result = torch.cat([cnn_result, pad_tensor], axis=1)\n",
    "            lstm_inputs.append(cnn_result)\n",
    "        lstm_inputs = torch.cat(lstm_inputs, dim=-1).reshape((batch, doc_len, self.lstm_embedding_size)) \n",
    "        lstm_inputs = lstm_inputs * torch.nn.functional.sigmoid(lstm_inputs)  # Swish \n",
    "        lstm_inputs = pack_padded_sequence(lstm_inputs, lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        return lstm_inputs\n",
    "\n",
    "\n",
    "class Document_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_size=350, config=config):\n",
    "        super(Document_Encoder, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding_size = embedding_size\n",
    "        self.doc_encoder = nn.LSTM(self.embedding_size, config.lstm_hidden, num_layers=1,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, lstm_inputs):\n",
    "        _, doc_encoder_out = self.doc_encoder(lstm_inputs)\n",
    "\n",
    "        return doc_encoder_out\n",
    "\n",
    "class Sentence_Extractor(nn.Module):\n",
    "    def __init__(self, embedding_size=350, config=config):\n",
    "        super(Sentence_Extractor, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sentence_extractor = nn.LSTM(self.embedding_size, config.lstm_hidden, num_layers=1,\n",
    "                                  bidirectional=True, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, lstm_inputs, encoder_in):\n",
    "        out_packed, (_, __) = self.sentence_extractor(lstm_inputs, encoder_in)\n",
    "        out, out_lens = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        out = self.dropout_layer(out)\n",
    "        return out\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, bert, config=config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = Bert_Embedding(bert, config=config)\n",
    "        self.doc_encoder = Document_Encoder(self.embeddings.lstm_embedding_size, config=config)\n",
    "        self.sentence_extractor = Sentence_Extractor(self.embeddings.lstm_embedding_size, config=config)\n",
    "\n",
    "        self.linear = Linear(config.lstm_hidden * 2, 1) \n",
    "        self.loss_func = nn.BCELoss()\n",
    "        self.loss_padding_value = -100\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, document_mask, attention_mask, y=None):\n",
    "        lstm_inputs = self.embeddings(x, document_mask, attention_mask)\n",
    "\n",
    "        doc_encoder_out = self.doc_encoder(lstm_inputs)  \n",
    "        encoder_in = doc_encoder_out\n",
    "\n",
    "        out = self.sentence_extractor(lstm_inputs, encoder_in)\n",
    "        out = self.sigmoid(self.linear(out).squeeze(-1))\n",
    "        # print(out.shape, mask.shape)\n",
    "        # out *= mask\n",
    "        \n",
    "        if y is not None:\n",
    "            y = pad_sequence(y, batch_first=True, padding_value=self.loss_padding_value).type(torch.FloatTensor).to(out.device)\n",
    "            loss = self.loss_func(out, y)\n",
    "            out = nn.functional.softmax(out, dim=-1)\n",
    "            return out, loss\n",
    "\n",
    "        return nn.functional.softmax(out, dim=-1)\n",
    "\n",
    "    def predict(self, tokenizer, doc):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test model\n",
    "# model = Model(bert).to(config.device)\n",
    "\n",
    "# for item in data_loader:\n",
    "#     ids = item['input_ids']\n",
    "#     document_mask = item['document_mask']\n",
    "#     attention_mask = item['attention_mask']\n",
    "#     print(model(ids, document_mask, attention_mask).shape, item['labels'].shape)\n",
    "#     break\n",
    "\n",
    "# config.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def torch_save(dir, save_dict):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    for name in save_dict:\n",
    "        torch.save(save_dict[name], os.path.join(dir, name + '.pt'))\n",
    "    \n",
    "def torch_load_all(dir):\n",
    "    save_dict = {}\n",
    "    for name in os.listdir(dir):\n",
    "        save_dict[name.replace('.pt', '')] = torch.load(os.path.join(dir, name))\n",
    "    \n",
    "    return save_dict\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer=None, scheduler=None, config=config, start_epoch=0):\n",
    "    model.train()\n",
    "    all_train_loss, all_dev_loss, all_dev_f1 = [], [], []\n",
    "    best_dev_loss = 1e9\n",
    "    best_dev_f1 = 0\n",
    "\n",
    "    print_freq = config.print_freq\n",
    "    batch_size = config.batch_size\n",
    "    epochs = config.num_epochs\n",
    "    gradient_accumulation_steps = config.gradient_accumulation_steps\n",
    "    save_dir = config.save_dir\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-3, epochs=epochs, steps_per_epoch=len(train_loader.dataset))\n",
    "    print_after = int(print_freq * len(train_loader.dataset) / batch_size)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print_counter = 0\n",
    "        total_loss = []\n",
    "        print('Epoch {} started on {}'.format(epoch, datetime.now().strftime('%d/%m/%Y %H:%M:%S')))\n",
    "        for step, item in enumerate(tqdm(train_loader)):\n",
    "            ids = item['input_ids']\n",
    "            document_mask = item['document_mask']\n",
    "            attention_mask = item['attention_mask']\n",
    "            label = item['labels']\n",
    "            logits, loss = model(ids, document_mask, attention_mask, y=label)\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            if (step+1) % gradient_accumulation_steps == 0 or step == len(train_loader.dataset)-1:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss.append(loss.item())\n",
    "            if step > print_counter:\n",
    "                print('Step: {}, loss: {}, total loss: {}'.format(step, loss.item(), np.mean(total_loss)))\n",
    "                print_counter += print_after\n",
    "        \n",
    "        print('Train loss:', np.mean(total_loss))\n",
    "        all_train_loss.append(total_loss)\n",
    "        result_dict = {'epoch': epoch, 'all_train_loss': all_train_loss}\n",
    "        model_name = 'model_{}.pt'.format(epoch)\n",
    "\n",
    "        save_dict = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            # 'optimizer_state_dict': optimizer.state_dict() if optimizer is not None else None,\n",
    "            # 'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'result_dict': result_dict\n",
    "            }\n",
    "        if val_loader is not None:\n",
    "            dev_loss, dev_f1 = eval(model, val_loader, config)\n",
    "            all_dev_loss.append(dev_loss)\n",
    "            all_dev_f1.append(dev_f1)\n",
    "            print('Dev loss: {}, Dev F1: {}'.format(dev_loss, dev_f1))\n",
    "\n",
    "            custom_save_dict = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config,\n",
    "                'result_dict': result_dict\n",
    "            }\n",
    "            if dev_loss < best_dev_loss:\n",
    "                torch_save(os.path.join(save_dir, 'best-model-loss'), custom_save_dict)\n",
    "                best_dev_loss = dev_loss\n",
    "            if dev_f1 > best_dev_f1:\n",
    "                torch_save(os.path.join(save_dir, 'best-model-f1'), custom_save_dict)\n",
    "                best_dev_f1 = dev_f1\n",
    "\n",
    "            result_dict.update ({\n",
    "                'all_dev_loss': all_dev_loss,\n",
    "                'all_dev_f1': all_dev_f1,\n",
    "                'best_dev_loss': best_dev_loss,\n",
    "                'best_dev_f1': best_dev_f1\n",
    "                })\n",
    "\n",
    "        \n",
    "        torch_save(os.path.join(save_dir, 'checkpoint_{}'.format(epoch)), save_dict)\n",
    "\n",
    "        # torch_save(os.path.join(save_dir, 'model_{}.pt'.format(str(epoch))),\n",
    "        #            model, config, epoch, optimizer, scheduler, all_train_loss, all_dev_loss, best_dev_loss)\n",
    "        print('Finish epoch {} on {}. \\n'.format(epoch, datetime.now().strftime('%d/%m/%Y %H:%M:%S')))\n",
    " \n",
    "\n",
    "def eval(model, val_loader, config=config, get_report=True):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in val_loader:\n",
    "            ids = item['input_ids']\n",
    "            document_mask = item['document_mask']\n",
    "            attention_mask = item['attention_mask']\n",
    "            labels = item['labels']\n",
    "            logits, loss = model(ids, document_mask, attention_mask, y=labels)\n",
    "            \n",
    "            prob = logits\n",
    "            batch_y_true = []\n",
    "            batch_y_pred = [[] for i in range(len(labels))]\n",
    "            for j, sent in enumerate(labels):\n",
    "                last_index = len(labels[j])\n",
    "                batch_y_true.extend(labels[j])\n",
    "                temp_prob = np.argsort(prob[j, :last_index].tolist())\n",
    "                batch_y_pred[j] = [0] * len(sent)\n",
    "                # Get top 4 best sentence\n",
    "                for k in temp_prob[-4:]:\n",
    "                    batch_y_pred[j][k] = 1\n",
    "            y_true.extend(batch_y_true)\n",
    "            for sent in batch_y_pred:\n",
    "                y_pred.extend(sent)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "    if get_report:\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "    model.train()    \n",
    "    \n",
    "    return np.mean(total_loss), f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(bert).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ESDataset(val_encodings[:100], val_labels, val_keys)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "train_dataset = ESDataset(train_encodings[:10000], train_labels, train_keys)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict = torch_load_all('save/cnn/best-model-f1')\n",
    "\n",
    "model = Model(bert, config).to(config.device)\n",
    "model.load_state_dict(final_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# model.load_state_dict(torch.load(ROOT_DIR+'/save/best-model.pt'))\n",
    "# model.to('cuda')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "alpha = 0.9\n",
    "\n",
    "\n",
    "def get_test_loader(docs, config=config):\n",
    "    encodings = []\n",
    "    for doc in docs:\n",
    "        encodings.append(tokenizer(doc[:config.MAX_DOC_LEN], truncation=True,\n",
    "                                   max_length=config.MAX_SEQ_LEN, padding='max_length'))\n",
    "    \n",
    "    test_dataset = ESDataset(encodings)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def get_all_probs(model, all_doc, config=config):\n",
    "    res = []\n",
    "    test_loader = get_test_loader(all_doc, config=config)\n",
    "    for item in tqdm(test_loader):\n",
    "        ids = item['input_ids'].to(config.device)\n",
    "        document_mask = item['document_mask'].to(config.device)\n",
    "        attention_mask = item['attention_mask'].to(config.device)\n",
    "        prob = model(ids, document_mask, attention_mask)[0].tolist()\n",
    "        res.append(prob)\n",
    "            \n",
    "    return res\n",
    "\n",
    "def calculateSimilarity(sentence, doc):\n",
    "    score = scorer.score('\\n'.join(doc), sentence)\n",
    "    return np.mean([score['rouge2'][2], score['rougeLsum'][2]])\n",
    "\n",
    "def choose_summary_mmr(doc, prob, k=3):\n",
    "    prob = np.array(prob)\n",
    "    idx = [np.argmax(prob)]\n",
    "    prob[idx[0]] = 0\n",
    "    summary = [doc[idx[0]]]\n",
    "\n",
    "    while len(idx) < min(k, len(doc)):\n",
    "        mmr = -100 * np.ones_like(prob)\n",
    "        for i, sent in enumerate(doc):\n",
    "            if prob[i] != 0:\n",
    "                mmr[i] = alpha * prob[i] - (1-alpha) * calculateSimilarity(sent, summary)\n",
    "        pos = np.argmax(mmr)\n",
    "        prob[pos] = 0\n",
    "        summary.append(doc[pos])\n",
    "        idx.append(pos)\n",
    "    summary = sorted(list(zip(idx, summary)), key=lambda x: x[0])\n",
    "    return [x[1] for x in summary]\n",
    "\n",
    "def choose_summary(doc, prob, k=3):\n",
    "    idx = torch.topk(torch.tensor(prob), k=k).indices.tolist()\n",
    "    return [doc[i] for i in sorted(idx)]\n",
    "\n",
    "\n",
    "def test(documents, summaries, all_probs, choose_summary=choose_summary, k=3, save_dir='./'):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    res = {'rouge1': [], 'rouge2': [], 'rougeLsum': []}\n",
    "    for i, document in enumerate(tqdm(documents)):\n",
    "        processed_document = document[: len(all_probs[i])]\n",
    "        score = scorer.score('\\n'.join(summaries[i]), '\\n'.join(choose_summary(processed_document, all_probs[i], k))) # target, prediction\n",
    "        for cate in res:\n",
    "            res[cate].append(score[cate][2])  # f1 score\n",
    "        # print(i, [res[cate][-1] for cate in res])\n",
    "    print('\\n\\nResult :')\n",
    "    for i in res:\n",
    "        x = np.mean(res[i])\n",
    "        print(i, x)\n",
    "        res[i].extend([0.10101, x])\n",
    "    # res['doc_id'] = list(range(len(documents))) + ['0.10101', 'None']\n",
    "\n",
    "    # df = pd.DataFrame.from_dict(res)\n",
    "    # df.to_csv(os.path.join(save_dir, 'result200.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1093 [00:00<?, ?it/s]/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 1093/1093 [04:51<00:00,  3.75it/s]\n",
      "100%|██████████| 1093/1093 [00:04<00:00, 225.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Result :\n",
      "rouge1 0.3102072385848451\n",
      "rouge2 0.12535380813127672\n",
      "rougeLsum 0.2760824764243371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts, test_dict_labels, test_summaries = load_text(config.test_data_dir, return_sum=True)\n",
    "model.to(config.device)\n",
    "\n",
    "test_ids = list(test_texts.keys())\n",
    "docs = [test_texts[i] for i in test_ids]\n",
    "summaries = [test_summaries[i] for i in test_ids]\n",
    "probs = get_all_probs(model, docs)\n",
    "test(docs, summaries, probs, choose_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.09986606240272522,\n",
       "  0.07213430106639862,\n",
       "  0.09388574957847595,\n",
       "  0.07409825921058655,\n",
       "  0.18170563876628876,\n",
       "  0.11890880763530731,\n",
       "  0.08771233260631561,\n",
       "  0.07143262028694153,\n",
       "  0.13012860715389252,\n",
       "  0.07012753933668137]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62257e83c93fa3387442bd5aebc80bdf7ed58eaa472b333d9af121b22db2a637"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
