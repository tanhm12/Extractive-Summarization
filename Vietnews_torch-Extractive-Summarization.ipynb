{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 10:47:04.750820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64\n",
      "2021-10-11 10:47:04.750856: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch \n",
    "import time\n",
    "\n",
    "from underthesea import sent_tokenize as sent_tokenize_uts, word_tokenize as word_tokenize_uts\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import LSTM, Conv2d, Linear\n",
    "from torch.nn.functional import max_pool2d\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sống với nhau như vợ_chồng nhưng không đăng_ký kết_hôn , đến khi chia_tay , Cường giành quyền nuôi con không được nên đã mang xăng đến cửa_hàng của “ vợ ” để đốt .',\n",
       "  'Đối_tượng đang bị cơ_quan công_an truy bắt .'],\n",
       " 'Sống với nhau như vợ_chồng nhưng không đăng_ký kết_hôn , đến khi chia_tay , Cường giành quyền nuôi con không được nên đã mang xăng đến cửa_hàng của “ vợ ” để đốt . Đối_tượng đang bị cơ_quan công_an truy bắt .')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Sống với nhau như vợ_chồng nhưng không đăng_ký kết_hôn , đến khi chia_tay , Cường giành quyền nuôi con không được nên đã mang xăng đến cửa_hàng của “ vợ ” để đốt . Đối_tượng đang bị cơ_quan công_an truy bắt .'\n",
    "sent_tokenize_uts(text), word_tokenize_uts(text, format='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/vietnews/data'\n",
    "\n",
    "def get_files(name):\n",
    "    dir = os.path.join(base_dir, name)\n",
    "    return [os.path.join(dir, file) for file in os.listdir(dir)]\n",
    "    \n",
    "train_files = get_files('train_tokenized')\n",
    "valid_files = get_files('val_tokenized')\n",
    "test_files = get_files( 'test_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER = False\n",
    "LENGTH_THRESHOLD = 15\n",
    "rouge_factors = {'rouge1': 0.3, 'rouge2': 0.3, 'rougeL': 0.4}  \n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    return sent_tokenize_uts(doc)\n",
    "\n",
    "def word_tokenize(text, format='list'):\n",
    "    return word_tokenize_uts(text, format=format)\n",
    "\n",
    "def parse_file(file):\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        document = f.read().rstrip().split(\"\\n\\n\")\n",
    "    for i in range(len(document)):\n",
    "        document[i] = [text for text in document[i].split('\\n') if len(text) > 0]\n",
    "        \n",
    "    if len(document) < 4:\n",
    "        title, summary, doc = document \n",
    "        img_caption = ''\n",
    "    else:\n",
    "        title, summary, doc, img_caption = document \n",
    "    summary = sent_tokenize(summary[0])\n",
    "    return title, summary, doc, img_caption\n",
    "\n",
    "\n",
    "def make_label(doc, sum, scorer):\n",
    "    doc_size = len(doc)\n",
    "    res = [0] * doc_size\n",
    "    n = min(len(sum), doc_size)\n",
    "    # f1 of rouge-L\n",
    "    for j in range(n):\n",
    "        score = [scorer.score(sum[j], sent_i) for sent_i in doc]\n",
    "        score = [( \n",
    "            x['rouge1'][2] * rouge_factors['rouge1'] + \\\n",
    "            x['rouge2'][2] * rouge_factors['rouge2'] + \\\n",
    "            x['rougeL'][2] * rouge_factors['rougeL']\n",
    "            ) for x in score]\n",
    "        sent_pos = np.argmax(score)\n",
    "        for i in range(doc_size):\n",
    "            if res[sent_pos] == 1:\n",
    "                score[sent_pos] = 0\n",
    "                sent_pos = np.argmax(score)\n",
    "            else:\n",
    "                res[sent_pos] = 1\n",
    "                break\n",
    "        # print(score[sent_pos])\n",
    "        # print(doc[sent_pos])\n",
    "        # print(sum[j], \"\\n\")\n",
    "    return res\n",
    "\n",
    "def process(files):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "    img_captions = {}\n",
    "    titles = {}\n",
    "    docs = {}\n",
    "    summaries = {}\n",
    "    labels = {}\n",
    "    remove_files = []\n",
    "    for idx in tqdm(range(len(files))):\n",
    "        # if idx%1000 == 0:\n",
    "        #     print('\\n', os.getpid(), idx)\n",
    "        title, summary, doc, img_caption = parse_file(os.path.join(files[idx]))\n",
    "        # print(title, summary, doc, img_caption)\n",
    "        if len(doc) < len(summary) or len(doc) == 0 or len(summary) == 0:\n",
    "            remove_files.append(files[idx])   \n",
    "            continue    \n",
    "        label = make_label(doc, summary, scorer)\n",
    "        titles[files[idx]] = title\n",
    "        docs[files[idx]] = doc\n",
    "        labels[files[idx]] = label\n",
    "        summaries[files[idx]] = summary\n",
    "        img_captions[files[idx]] = img_caption\n",
    "        # if idx%5000 == 0:\n",
    "        #     a = list(zip(label, doc))\n",
    "        #     for i in a:\n",
    "        #         print(len(i[1]), i[0], i[1])\n",
    "        #     print('##########\\n','\\n'.join(summary))\n",
    "    return titles, docs, labels, summaries, img_captions, remove_files\n",
    "\n",
    "def json_dump(obj, file):\n",
    "    with open(file, 'w', encoding='utf8') as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def process_and_write(files, write_dir):\n",
    "    titles, docs, labels, summaries, img_captions, remove_files = process(files)\n",
    "\n",
    "    os.makedirs(write_dir, exist_ok=True)\n",
    "    json_dump(docs, os.path.join(write_dir, 'docs.json'))\n",
    "    json_dump(labels, os.path.join(write_dir, 'labels.json'))\n",
    "    json_dump(summaries, os.path.join(write_dir, 'summaries.json'))\n",
    "    json_dump(titles, os.path.join(write_dir, 'titles.json'))\n",
    "    json_dump(img_captions, os.path.join(write_dir, 'img_captions.json'))\n",
    "    json_dump(remove_files, os.path.join(write_dir, 'remove_files.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.14it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 33.50it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.16it/s]\n"
     ]
    }
   ],
   "source": [
    "base_write_dir = 'data/viet_new_processed'\n",
    "process_and_write(valid_files[:100], os.path.join(base_write_dir, 'valid'))\n",
    "process_and_write(test_files[:100], os.path.join(base_write_dir, 'test'))\n",
    "process_and_write(train_files[:100], os.path.join(base_write_dir, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([84, 15,  1]), array([1., 2., 3., 4.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('data/viet_new_processed/train/labels.json') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "labels = list(labels.values())\n",
    "length = [np.sum(x) for x in labels]\n",
    "print(length)\n",
    "np.histogram(length, bins=len(set(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnewsConfig:\n",
    "    def __init__(self):\n",
    "        self.train_data_dir = 'data/viet_new_processed/train'\n",
    "        self.val_data_dir = 'data/viet_new_processed/valid'\n",
    "        self.test_data_dir = 'data/viet_new_processed/test'\n",
    "\n",
    "        self.bert_type = RobertaModel\n",
    "        self.tokenizer_type = RobertaTokenizerFast\n",
    "        self.bert_name = 'Zayt/viRoberta-l6-h384-word-cased'  # my Roberta pretrained model on  4gb of text\n",
    "\n",
    "        self.MAX_SEQ_LEN = 96\n",
    "        self.MAX_DOC_LEN = 32\n",
    "\n",
    "        self.bert_hidden = 384\n",
    "        self.bert_n_layers = 3\n",
    "\n",
    "        self.windows_size = [1, 3, 5, 10]\n",
    "        self.out_channels = 50\n",
    "        self.lstm_hidden = 256\n",
    "        self.device = 'cuda:3'\n",
    "\n",
    "        self.batch_size = 6\n",
    "        self.num_epochs = 7\n",
    "        self.warmup_steps = 500\n",
    "        self.gradient_accumulation_steps = 16\n",
    "        self.print_freq = 0.05\n",
    "        self.save_dir = './save/vietnews'\n",
    "\n",
    "config = VietnewsConfig()\n",
    "# config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = config.tokenizer_type.from_pretrained(config.bert_name)\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_text(dir, return_sum=False):\n",
    "    docs = load_json(os.path.join(dir, 'docs.json'))\n",
    "    labels = load_json(os.path.join(dir, 'labels.json'))\n",
    "    if return_sum:\n",
    "        summaries = load_json(os.path.join(dir, 'summaries.json'))\n",
    "        return docs, labels, summaries\n",
    "    return docs, labels\n",
    "\n",
    "def get_encodings(docs, labels, config=config):\n",
    "    keys = list(docs.keys())\n",
    "    encodings = []\n",
    "    return_labels = []\n",
    "\n",
    "    for k in tqdm(keys):\n",
    "        encodings.append(tokenizer(docs[k][:config.MAX_DOC_LEN], truncation=True,\n",
    "                                   max_length=config.MAX_SEQ_LEN, padding='max_length',\n",
    "                                   ))\n",
    "        return_labels.append(labels[k][:config.MAX_DOC_LEN])\n",
    "    \n",
    "    return keys, encodings, return_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None, keys=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.keys = keys\n",
    "        self.encoding_keys = ['input_ids', 'attention_mask']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(self.encodings[idx][key]) for key in self.encoding_keys}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "def collate_fn(data):\n",
    "    keys = data[0].keys()\n",
    "\n",
    "    result = {k: [item[k] for item in data] for k in keys}\n",
    "    input_ids = result['input_ids']\n",
    "    result['document_mask'] = [torch.tensor([1] * len(input_ids[i])) for i in range(len(input_ids))]\n",
    "    \n",
    "    for k in result:\n",
    "        if k != 'labels':\n",
    "            result[k] = pad_sequence(result[k], batch_first=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 584/105400 [00:07<22:23, 78.01it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7647/1256000515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dict_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dict_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dict_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7647/2672333238.py\u001b[0m in \u001b[0;36mget_encodings\u001b[0;34m(docs, labels, config)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         encodings.append(tokenizer(docs[k][:config.MAX_DOC_LEN], truncation=True,\n\u001b[0;32m---> 22\u001b[0;31m                                    \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                                    ))\n\u001b[1;32m     24\u001b[0m         \u001b[0mreturn_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_DOC_LEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2415\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2417\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2418\u001b[0m             )\n\u001b[1;32m   2419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2600\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2601\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2602\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2603\u001b[0m         )\n\u001b[1;32m   2604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         ]\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         ]\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"special_tokens_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_texts, train_dict_labels = load_text(config.train_data_dir)\n",
    "val_texts, val_dict_labels = load_text(config.val_data_dir)\n",
    "\n",
    "train_keys, train_encodings, train_labels = get_encodings(train_texts, train_dict_labels)\n",
    "val_keys, val_encodings, val_labels = get_encodings(val_texts, val_dict_labels)\n",
    "\n",
    "# dataset = ESDataset(val_encodings, val_labels, val_keys)\n",
    "# val_loader = DataLoader(dataset, batch_size=3, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0., 361157.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.]),\n",
       " array([  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,  96,\n",
       "        104, 112, 120, 128, 136, 144, 152]),\n",
       " <a list of 19 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "length = []\n",
    "for encoding in val_encodings:\n",
    "    length.extend([len(sent) for sent in encoding['input_ids']])\n",
    "# np.histogram(length, bins=[8*i for i in range(0, 20)])\n",
    "plt.hist(length, bins=[8*i for i in range(0, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b1ce4b9ecf4ce99b4848c604c3b829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/703 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425782b34be748648e90dd1f0e589629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/86.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Zayt/viRoberta-l6-h384-word-cased were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at Zayt/viRoberta-l6-h384-word-cased and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = config.bert_type.from_pretrained(config.bert_name)\n",
    "tokenizer = config.tokenizer_type.from_pretrained(config.bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Embedding(nn.Module):\n",
    "    def __init__(self, bert: RobertaModel, config: VietnewsConfig):\n",
    "        super(Bert_Embedding, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.bert_hidden = config.bert_hidden * config.bert_n_layers\n",
    "        self.get_n_layers = config.bert_n_layers\n",
    "        self.config = config\n",
    "        \n",
    "        self.windows_size = config.windows_size\n",
    "        self.out_channels = config.out_channels\n",
    "        self.lstm_embedding_size = len(self.windows_size) * config.MAX_SEQ_LEN  \n",
    "        self.filters = nn.ModuleList([nn.Conv2d(1, self.out_channels,\n",
    "                                                (i, self.bert_hidden)) for i in self.windows_size])\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, document_mask, attention_mask):\n",
    "        lens = [mask_i.sum().item() for mask_i in document_mask]\n",
    "\n",
    "        batch, doc_len, seq_len = list(x.shape)\n",
    "        x = x.reshape((batch*doc_len, seq_len))\n",
    "        attention_mask = attention_mask.reshape((batch*doc_len, seq_len))        \n",
    "\n",
    "        last_hds, pooler_output, hidden_states = self.bert(x, attention_mask, output_hidden_states=True, return_dict=False)\n",
    "        embeddings = torch.cat(hidden_states[-self.get_n_layers:], axis=-1)  # batch, doc_len, seq_len, self.bert_hidden\n",
    "        # print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape((batch * doc_len, 1,  seq_len, self.bert_hidden))  # batch * doc_len, 1, MAX_SEQ_LEN, bert_hidden\n",
    "        lstm_inputs = []\n",
    "\n",
    "        for i in range(len(self.windows_size)):\n",
    "            temp_out = self.filters[i](embeddings).squeeze(-1)  # batch * doc_len, self.out_channels, MAX_SEQ_LEN - self.windows_size[i] + 1\n",
    "            cnn_result = torch.mean(temp_out, dim=1) # average along out_channels axis\n",
    "            if cnn_result.shape[1] < self.config.MAX_SEQ_LEN: # pad cnn_result to MAX_SEQ_LEN\n",
    "                pad_tensor = torch.zeros((cnn_result.shape[0], self.config.MAX_SEQ_LEN - cnn_result.shape[1])).to(cnn_result.device)\n",
    "                cnn_result = torch.cat([cnn_result, pad_tensor], axis=1)\n",
    "            lstm_inputs.append(cnn_result)\n",
    "        lstm_inputs = torch.cat(lstm_inputs, dim=-1).reshape((batch, doc_len, self.lstm_embedding_size)) \n",
    "        lstm_inputs = lstm_inputs * torch.nn.functional.sigmoid(lstm_inputs)  # Swish \n",
    "        lstm_inputs = pack_padded_sequence(lstm_inputs, lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        return lstm_inputs\n",
    "\n",
    "\n",
    "class Document_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_size, config: VietnewsConfig):\n",
    "        super(Document_Encoder, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding_size = embedding_size\n",
    "        self.doc_encoder = nn.LSTM(self.embedding_size, config.lstm_hidden, num_layers=1,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, lstm_inputs):\n",
    "        _, doc_encoder_out = self.doc_encoder(lstm_inputs)\n",
    "\n",
    "        return doc_encoder_out\n",
    "\n",
    "class Sentence_Extractor(nn.Module):\n",
    "    def __init__(self, embedding_size, config: VietnewsConfig):\n",
    "        super(Sentence_Extractor, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sentence_extractor = nn.LSTM(self.embedding_size, config.lstm_hidden, num_layers=1,\n",
    "                                  bidirectional=True, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, lstm_inputs, encoder_in):\n",
    "        out_packed, (_, __) = self.sentence_extractor(lstm_inputs, encoder_in)\n",
    "        out, out_lens = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        out = self.dropout_layer(out)\n",
    "        return out\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, bert, config: VietnewsConfig):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = Bert_Embedding(bert, config=config)\n",
    "        self.doc_encoder = Document_Encoder(self.embeddings.lstm_embedding_size, config=config)\n",
    "        self.sentence_extractor = Sentence_Extractor(self.embeddings.lstm_embedding_size, config=config)\n",
    "\n",
    "        self.linear = Linear(config.lstm_hidden * 2, 1) \n",
    "        self.loss_func = nn.BCELoss()\n",
    "        self.loss_padding_value = -100\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, document_mask, attention_mask, y=None):\n",
    "        lstm_inputs = self.embeddings(x, document_mask, attention_mask)\n",
    "\n",
    "        doc_encoder_out = self.doc_encoder(lstm_inputs)  \n",
    "        encoder_in = doc_encoder_out\n",
    "\n",
    "        out = self.sentence_extractor(lstm_inputs, encoder_in)\n",
    "        out = self.sigmoid(self.linear(out).squeeze(-1))\n",
    "        # print(out.shape, mask.shape)\n",
    "        # out *= mask\n",
    "        \n",
    "        if y is not None:\n",
    "            y = pad_sequence(y, batch_first=True, padding_value=self.loss_padding_value).type(torch.FloatTensor).to(out.device)\n",
    "            loss = self.loss_func(out, y)\n",
    "            out = nn.functional.softmax(out, dim=-1)\n",
    "            return out, loss\n",
    "\n",
    "        return nn.functional.softmax(out, dim=-1)\n",
    "\n",
    "    def predict(self, tokenizer, doc):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 23])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "## Test model\n",
    "# model = Model(bert, config).to(config.device)\n",
    "\n",
    "for item in val_loader:\n",
    "    ids = item['input_ids'].to(config.device)\n",
    "    document_mask = item['document_mask'].to(config.device)\n",
    "    attention_mask = item['attention_mask'].to(config.device)\n",
    "    print(model(ids, document_mask, attention_mask).shape)\n",
    "    break\n",
    "\n",
    "# config.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def torch_save(dir, save_dict):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    for name in save_dict:\n",
    "        torch.save(save_dict[name], os.path.join(dir, name + '.pt'))\n",
    "    \n",
    "def torch_load_all(dir):\n",
    "    save_dict = {}\n",
    "    for name in os.listdir(dir):\n",
    "        save_dict[name.replace('.pt', '')] = torch.load(os.path.join(dir, name), map_location=torch.device('cpu'))\n",
    "\n",
    "    return save_dict\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer=None, scheduler=None, config=config, start_epoch=0):\n",
    "    model.train()\n",
    "    all_train_loss, all_dev_loss, all_dev_f1 = [], [], []\n",
    "    best_dev_loss = 1e9\n",
    "    best_dev_f1 = 0\n",
    "\n",
    "    print_freq = config.print_freq\n",
    "    batch_size = config.batch_size\n",
    "    epochs = config.num_epochs\n",
    "    gradient_accumulation_steps = config.gradient_accumulation_steps\n",
    "    save_dir = config.save_dir\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-3, epochs=epochs, steps_per_epoch=len(train_loader.dataset))\n",
    "    print_after = int(print_freq * len(train_loader.dataset) / batch_size)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print_counter = 0\n",
    "        total_loss = []\n",
    "        print('Epoch {} started on {}'.format(epoch, datetime.now().strftime('%d/%m/%Y %H:%M:%S')))\n",
    "        for step, item in enumerate(tqdm(train_loader)):\n",
    "            ids = item['input_ids'].to(config.device)\n",
    "            document_mask = item['document_mask'].to(config.device)\n",
    "            attention_mask = item['attention_mask'].to(config.device)\n",
    "            label = item['labels']\n",
    "            logits, loss = model(ids, document_mask, attention_mask, y=label)\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            if (step+1) % gradient_accumulation_steps == 0 or step == len(train_loader.dataset)-1:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss.append(loss.item())\n",
    "            if step > print_counter:\n",
    "                print('Step: {}, loss: {}, total loss: {}'.format(step, loss.item(), np.mean(total_loss)))\n",
    "                print_counter += print_after\n",
    "        \n",
    "        print('Train loss:', np.mean(total_loss))\n",
    "        all_train_loss.append(total_loss)\n",
    "        result_dict = {'epoch': epoch, 'all_train_loss': all_train_loss}\n",
    "        model_name = 'model_{}.pt'.format(epoch)\n",
    "\n",
    "        save_dict = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            # 'optimizer_state_dict': optimizer.state_dict() if optimizer is not None else None,\n",
    "            # 'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'result_dict': result_dict\n",
    "            }\n",
    "        if val_loader is not None:\n",
    "            dev_loss, dev_f1 = eval(model, val_loader, config)\n",
    "            all_dev_loss.append(dev_loss)\n",
    "            all_dev_f1.append(dev_f1)\n",
    "            print('Dev loss: {}, Dev F1: {}'.format(dev_loss, dev_f1))\n",
    "\n",
    "            custom_save_dict = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config,\n",
    "                'result_dict': result_dict\n",
    "            }\n",
    "            if dev_loss < best_dev_loss:\n",
    "                torch_save(os.path.join(save_dir, 'best-model-loss'), custom_save_dict)\n",
    "                best_dev_loss = dev_loss\n",
    "            if dev_f1 > best_dev_f1:\n",
    "                torch_save(os.path.join(save_dir, 'best-model-f1'), custom_save_dict)\n",
    "                best_dev_f1 = dev_f1\n",
    "\n",
    "            result_dict.update ({\n",
    "                'all_dev_loss': all_dev_loss,\n",
    "                'all_dev_f1': all_dev_f1,\n",
    "                'best_dev_loss': best_dev_loss,\n",
    "                'best_dev_f1': best_dev_f1\n",
    "                })\n",
    "\n",
    "        \n",
    "        torch_save(os.path.join(save_dir, 'checkpoint_{}'.format(epoch)), save_dict)\n",
    "\n",
    "        # torch_save(os.path.join(save_dir, 'model_{}.pt'.format(str(epoch))),\n",
    "        #            model, config, epoch, optimizer, scheduler, all_train_loss, all_dev_loss, best_dev_loss)\n",
    "        print('Finish epoch {} on {}. \\n'.format(epoch, datetime.now().strftime('%d/%m/%Y %H:%M:%S')))\n",
    " \n",
    "\n",
    "def eval(model, val_loader, config=config, get_report=True):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in val_loader:\n",
    "            ids = item['input_ids'].to(config.device)\n",
    "            document_mask = item['document_mask'].to(config.device)\n",
    "            attention_mask = item['attention_mask'].to(config.device)\n",
    "            labels = item['labels']\n",
    "            logits, loss = model(ids, document_mask, attention_mask, y=labels)\n",
    "            \n",
    "            prob = logits\n",
    "            batch_y_true = []\n",
    "            batch_y_pred = [[] for i in range(len(labels))]\n",
    "            for j, sent in enumerate(labels):\n",
    "                last_index = len(labels[j])\n",
    "                batch_y_true.extend(labels[j])\n",
    "                temp_prob = np.argsort(prob[j, :last_index].tolist())\n",
    "                batch_y_pred[j] = [0] * len(sent)\n",
    "                # Get top 4 best sentence\n",
    "                for k in temp_prob[-4:]:\n",
    "                    batch_y_pred[j][k] = 1\n",
    "            y_true.extend(batch_y_true)\n",
    "            for sent in batch_y_pred:\n",
    "                y_pred.extend(sent)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "    if get_report:\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "    model.train()    \n",
    "    \n",
    "    return np.mean(total_loss), f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(bert).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ESDataset(val_encodings[:100], val_labels, val_keys)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "train_dataset = ESDataset(train_encodings[:10000], train_labels, train_keys)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict = torch_load_all('save/vietnews/best-model-f1')\n",
    "\n",
    "model = Model(bert, config).to(config.device)\n",
    "model.load_state_dict(final_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# model.load_state_dict(torch.load(ROOT_DIR+'/save/best-model.pt'))\n",
    "# model.to('cuda')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "alpha = 0.9\n",
    "\n",
    "\n",
    "def get_test_loader(docs, config=config):\n",
    "    encodings = []\n",
    "    for doc in docs:\n",
    "        encodings.append(tokenizer(doc[:config.MAX_DOC_LEN], truncation=True,\n",
    "                                   max_length=config.MAX_SEQ_LEN, padding='max_length'))\n",
    "    \n",
    "    test_dataset = ESDataset(encodings)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def get_all_probs(model, all_doc, config=config):\n",
    "    res = []\n",
    "    test_loader = get_test_loader(all_doc, config=config)\n",
    "    for item in tqdm(test_loader):\n",
    "        ids = item['input_ids'].to(config.device)\n",
    "        document_mask = item['document_mask'].to(config.device)\n",
    "        attention_mask = item['attention_mask'].to(config.device)\n",
    "        prob = model(ids, document_mask, attention_mask)[0].tolist()\n",
    "        res.append(prob)\n",
    "            \n",
    "    return res\n",
    "\n",
    "def calculateSimilarity(sentence, doc):\n",
    "    score = scorer.score('\\n'.join(doc), sentence)\n",
    "    return np.mean([score['rouge2'][2], score['rougeLsum'][2]])\n",
    "\n",
    "def choose_summary_mmr(doc, prob, k=3):\n",
    "    prob = np.array(prob)\n",
    "    idx = [np.argmax(prob)]\n",
    "    prob[idx[0]] = 0\n",
    "    summary = [doc[idx[0]]]\n",
    "\n",
    "    while len(idx) < min(k, len(doc)):\n",
    "        mmr = -100 * np.ones_like(prob)\n",
    "        for i, sent in enumerate(doc):\n",
    "            if prob[i] != 0:\n",
    "                mmr[i] = alpha * prob[i] - (1-alpha) * calculateSimilarity(sent, summary)\n",
    "        pos = np.argmax(mmr)\n",
    "        prob[pos] = 0\n",
    "        summary.append(doc[pos])\n",
    "        idx.append(pos)\n",
    "    summary = sorted(list(zip(idx, summary)), key=lambda x: x[0])\n",
    "    return [x[1] for x in summary]\n",
    "\n",
    "def choose_summary(doc, prob, k=3):\n",
    "    idx = torch.topk(torch.tensor(prob), k=k).indices.tolist()\n",
    "    return [doc[i] for i in sorted(idx)]\n",
    "\n",
    "\n",
    "def test(documents, summaries, all_probs, choose_summary=choose_summary, k=3, save_dir='./'):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    res = {'rouge1': [], 'rouge2': [], 'rougeLsum': []}\n",
    "    for i, document in enumerate(tqdm(documents)):\n",
    "        processed_document = document[: len(all_probs[i])]\n",
    "        score = scorer.score('\\n'.join(summaries[i]), '\\n'.join(choose_summary(processed_document, all_probs[i], k))) # target, prediction\n",
    "        for cate in res:\n",
    "            res[cate].append(score[cate][2])  # f1 score\n",
    "        # print(i, [res[cate][-1] for cate in res])\n",
    "    print('\\n\\nResult :')\n",
    "    for i in res:\n",
    "        x = np.mean(res[i])\n",
    "        print(i, x)\n",
    "        res[i].extend([0.10101, x])\n",
    "    # res['doc_id'] = list(range(len(documents))) + ['0.10101', 'None']\n",
    "\n",
    "    # df = pd.DataFrame.from_dict(res)\n",
    "    # df.to_csv(os.path.join(save_dir, 'result200.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1093 [00:00<?, ?it/s]/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 1093/1093 [04:51<00:00,  3.75it/s]\n",
      "100%|██████████| 1093/1093 [00:04<00:00, 225.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Result :\n",
      "rouge1 0.3102072385848451\n",
      "rouge2 0.12535380813127672\n",
      "rougeLsum 0.2760824764243371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts, test_dict_labels, test_summaries = load_text(config.test_data_dir, return_sum=True)\n",
    "model.to(config.device)\n",
    "\n",
    "test_ids = list(test_texts.keys())\n",
    "docs = [test_texts[i] for i in test_ids]\n",
    "summaries = [test_summaries[i] for i in test_ids]\n",
    "probs = get_all_probs(model, docs)\n",
    "test(docs, summaries, probs, choose_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.09986606240272522,\n",
       "  0.07213430106639862,\n",
       "  0.09388574957847595,\n",
       "  0.07409825921058655,\n",
       "  0.18170563876628876,\n",
       "  0.11890880763530731,\n",
       "  0.08771233260631561,\n",
       "  0.07143262028694153,\n",
       "  0.13012860715389252,\n",
       "  0.07012753933668137]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62257e83c93fa3387442bd5aebc80bdf7ed58eaa472b333d9af121b22db2a637"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
